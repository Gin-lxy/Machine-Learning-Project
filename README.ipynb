{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deception Detection in Narrated Stories: A Deep Learning Approach\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This report presents an automated approach for detecting deception in\n",
    "narrated stories using deep learning techniques. We develop a novel\n",
    "audio-based deception detection system that combines mel-spectrograms\n",
    "and Mel-frequency cepstral coefficients (MFCCs) using a\n",
    "transformer-based architecture. The system processes audio recordings of\n",
    "3-5 minutes in duration and predicts whether the narrated story is true\n",
    "or false.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Deception detection in spoken narratives presents a challenging and\n",
    "important problem in various fields, including security, psychology, and\n",
    "human-computer interaction. Traditional approaches often rely on human\n",
    "expertise and are subject to bias and inconsistency. This project\n",
    "develops an automated system that can analyze audio recordings and\n",
    "determine the veracity of narrated stories.\n",
    "\n",
    "## 2. Methodology\n",
    "\n",
    "### 2.1 Data Processing Pipeline\n",
    "\n",
    "The system implements a comprehensive data processing pipeline\n",
    "consisting of three main stages:\n",
    "\n",
    "1.  **Audio Feature Extraction**:\n",
    "    -   Mel-spectrogram generation (128 mel bands)\n",
    "    -   MFCC extraction (20 coefficients)\n",
    "    -   Length standardization to 130 time steps\n",
    "2.  **Feature Transformation**:\n",
    "    -   Linear embedding of mel-spectrograms and MFCCs\n",
    "    -   Positional encoding for sequence information\n",
    "    -   Feature fusion through concatenation\n",
    "3.  **Classification**:\n",
    "    -   Transformer-based sequence processing\n",
    "    -   Global average pooling\n",
    "    -   Multi-layer classification head\n",
    "\n",
    "### 2.2 Model Architecture\n",
    "\n",
    "The core of our system is the AudioTransformer model, which consists of\n",
    "several key components:\n",
    "\n",
    "``` python\n",
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(self, mel_channels=128, mfcc_channels=20, d_model=256, nhead=8, \n",
    "                 num_layers=4, dim_feedforward=1024, dropout=0.1):\n",
    "        # Feature embedding layers\n",
    "        self.mel_embed = nn.Linear(mel_channels, d_model)\n",
    "        self.mfcc_embed = nn.Linear(mfcc_channels, d_model)\n",
    "        \n",
    "        # Positional encoding for sequence information\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Transformer encoder for sequence processing\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "```\n",
    "\n",
    "The model architecture incorporates several innovative design choices:\n",
    "\n",
    "1.  **Dual-stream Processing**: The model processes mel-spectrograms and\n",
    "    MFCCs in parallel streams before fusion, allowing it to capture both\n",
    "    spectral and cepstral information.\n",
    "\n",
    "2.  **Positional Encoding**: A sinusoidal positional encoding scheme\n",
    "    helps the model understand temporal relationships in the audio\n",
    "    features:\n",
    "\n",
    "``` python\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "```\n",
    "\n",
    "1.  **Multi-head Attention**: The transformer encoder uses 8 attention\n",
    "    heads to capture different types of relationships in the audio\n",
    "    features.\n",
    "\n",
    "### 2.3 Training Strategy\n",
    "\n",
    "The training process incorporates several techniques to improve model\n",
    "robustness:\n",
    "\n",
    "1.  **Learning Rate Management**:\n",
    "    -   Initial learning rate: 0.0001\n",
    "    -   Learning rate reduction on plateau\n",
    "    -   Patience of 5 epochs for learning rate adjustment\n",
    "2.  **Regularization**:\n",
    "    -   Dropout (0.1) in transformer layers and classification head\n",
    "    -   Gradient clipping with max norm 1.0\n",
    "    -   Early stopping with patience of 10 epochs\n",
    "3.  **Loss Function**:\n",
    "    -   Binary Cross Entropy Loss for binary classification\n",
    "    -   Model outputs probability through sigmoid activation\n",
    "\n",
    "## 3. Implementation Details\n",
    "\n",
    "The implementation includes several practical considerations for robust\n",
    "deployment:\n",
    "\n",
    "### 3.1 Audio Processing\n",
    "\n",
    "``` python\n",
    "def load_audio_features(file_path, fixed_length=130):\n",
    "    # Load and resample audio\n",
    "    y, sr = librosa.load(file_path)\n",
    "    \n",
    "    # Extract features\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "```\n",
    "\n",
    "### 3.2 Dataset Management\n",
    "\n",
    "A custom PyTorch dataset class handles data loading and batching:\n",
    "\n",
    "``` python\n",
    "class DeceptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features_list, labels):\n",
    "        self.features = features_list\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mel_spec = torch.FloatTensor(self.features[idx]['mel_spec']).unsqueeze(0)\n",
    "        mfcc = torch.FloatTensor(self.features[idx]['mfcc'])\n",
    "        label = torch.FloatTensor([self.labels[idx]])\n",
    "        return {'mel_spec': mel_spec, 'mfcc': mfcc, 'label': label}\n",
    "```\n",
    "\n",
    "### 3.3 Inference System\n",
    "\n",
    "The StoryPredictor class provides a clean interface for making\n",
    "predictions:\n",
    "\n",
    "``` python\n",
    "class StoryPredictor:\n",
    "    def __init__(self, model_path='best_model.pth'):\n",
    "        self.model = AudioTransformer()\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, audio_path):\n",
    "        features = self.process_audio(audio_path)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(mel_spec, mfcc)\n",
    "            probability = output.item()\n",
    "            prediction = probability > 0.5\n",
    "        return prediction, probability\n",
    "```\n",
    "\n",
    "## 4. Experimental Results and Analysis\n",
    "\n",
    "### 4.1 Training Dynamics\n",
    "\n",
    "The model was trained for 28 epochs before early stopping was triggered.\n",
    "The training process revealed several key observations:\n",
    "\n",
    "1.  **Loss Trends**:\n",
    "    -   Training loss showed consistent decrease from \\~0.70 to \\~0.28\n",
    "    -   Validation loss exhibited unstable behavior, increasing from\n",
    "        \\~0.70 to \\~1.0\n",
    "    -   Significant divergence between training and validation loss\n",
    "        after epoch 15\n",
    "2.  **Accuracy Performance**:\n",
    "    -   Validation accuracy fluctuated significantly between 0.40 and\n",
    "        0.70\n",
    "    -   Best validation accuracy achieved was 0.70 at epoch 5\n",
    "    -   No clear trend of improving accuracy over time\n",
    "    -   Final validation accuracy stabilized around 0.60\n",
    "3.  **Convergence Issues**:\n",
    "    -   Early stopping triggered at epoch 28 due to lack of improvement\n",
    "    -   Signs of overfitting emerged after epoch 15 with diverging\n",
    "        train/val losses\n",
    "    -   Model showed high variance in validation metrics\n",
    "\n",
    "### 4.2 Performance Analysis\n",
    "\n",
    "The experimental results highlight several challenges:\n",
    "\n",
    "1.  **Model Stability**: The high variance in validation accuracy\n",
    "    (ranging from 0.40 to 0.70) indicates stability issues in the\n",
    "    model’s predictions.\n",
    "\n",
    "2.  **Overfitting Indicators**:\n",
    "\n",
    "    -   Decreasing training loss while validation loss increases\n",
    "    -   Growing gap between training and validation metrics\n",
    "    -   Unstable validation accuracy despite improving training metrics\n",
    "\n",
    "3.  **Limited Generalization**: The model’s inability to achieve\n",
    "    consistent validation accuracy above 0.70 suggests limited\n",
    "    generalization capability.\n",
    "\n",
    "## 5. Technical Considerations and Limitations\n",
    "\n",
    "1.  **Fixed-length Processing**: The system standardizes all inputs to\n",
    "    130 time steps, which may lose information from longer recordings or\n",
    "    pad shorter ones.\n",
    "\n",
    "2.  **Memory Requirements**: The transformer architecture’s quadratic\n",
    "    attention complexity requires careful batch size management (set to\n",
    "    8 in implementation).\n",
    "\n",
    "3.  **GPU Acceleration**: The system is designed to utilize GPU\n",
    "    acceleration when available but can fall back to CPU processing.\n",
    "\n",
    "4.  **dataset**:The training set is too small, the training effect is\n",
    "    poor, there is no standard test set to verify the training effect of\n",
    "    the model.\n",
    "\n",
    "## 6. Future Improvements\n",
    "\n",
    "Several potential improvements could enhance the system:\n",
    "\n",
    "1.  **Variable Length Processing**: Implement dynamic padding and\n",
    "    masking to handle variable-length inputs more effectively.\n",
    "\n",
    "2.  **Additional Features**: Incorporate prosodic features like pitch\n",
    "    and energy variations.\n",
    "\n",
    "3.  **Cross-lingual Adaptation**: Develop language-specific models or\n",
    "    language-agnostic features for better cross-lingual performance.\n",
    "\n",
    "4.  **dataset**: Avoid overfitting by using different types of official\n",
    "    datasets that contain more data.\n",
    "\n",
    "5.  **Hyperparameters**: Hyperparameters need to be further adjusted by\n",
    "    further training \\#\\# 7. Conclusion\n",
    "\n",
    "This implementation demonstrates a practical approach to automated\n",
    "deception detection in narrated stories. The combination of spectral and\n",
    "cepstral features processed through a transformer architecture provides\n",
    "a robust foundation for this challenging task. While the system shows\n",
    "promise, careful consideration of its limitations and potential biases\n",
    "is essential for responsible deployment.\n",
    "\n",
    "## References\n",
    "\n",
    "1.  Vaswani, A., et al. (2017). Attention is all you need. NeurIPS.\n",
    "2.  McFee, B., et al. (2015). librosa: Audio and Music Signal Analysis\n",
    "    in Python.\n",
    "3.  PyTorch documentation and tutorials.\n",
    "4.  Muli,https://zh.d2l.ai/\n",
    "5.  https://github.com/datawhalechina/llms-from-scratch-cn by DataWhale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
